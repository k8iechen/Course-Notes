SUMMARY

![image-20200522084324289](C:\Users\Kaycee\AppData\Roaming\Typora\typora-user-images\image-20200522084324289.png)

## C3: PROBABILITY

1. [def] probability
   1. allows quantifiable variability in experimental outcome
   2. cannot be predicted with certainty
2. how is probability determined
3. principles of probability

### 3.1 Sample Spaces & Events

**sample space**: a set of possible outcomes

**discrete sample space**: has finitely many or countable infinity of elements

**continuous sample space**: sample space that constitute a continuum

**event**: subset of a sample

- **mutually exclusive events**: sets with no common elements

- **unions**: $A \cup B = S$

  subset S contains all elements in A AND B

- **intersections**: $A \cap B = S$

  subset S contains all elements COMMON to both A and B

- **complements**: $\bar{A} + A = S$

  subset of S containing all elements not in A

e.g. $\bar{(A \cup B)} =\bar{A} \cap \bar{B}$

![image-20200522070437169](C:\Users\Kaycee\AppData\Roaming\Typora\typora-user-images\image-20200522070437169.png)

### 3.2 Counting

[thm] **fundamental theorem of counting**: if sets $A_{1}, A_{2}, ..., A_{k}$ contain, respectively, $n_{1}, n_{2}, ..., n_{k}$ elements, there are $n_{1} \times n_{2} \times ... \times n_{k}$ ways of choosing the first element of $A_{1}$, then an element of $A_{2}, ..., A_{k}$.

**permutation**: the set of arrangements of when r objects are chosen from a set of n distinct objects

**factorial notation**: product of consecutive integers
$$
n!=n(n-1)(n-2)...2\cdot1 \\
0! = 1
$$
[thm] number of **permutations** of r objects selected from a set of n distinct objects is
$$
\begin{align*}
_{n}P_{r}&=n(n-1)(n-2)...(n-r+1) \\
&= \frac{n!}{(n-r)!} \tag{3.2-1} \\ 
\end{align*}
$$
for $r=1,2,...,n$

[thm] number of **combinations** of n objects taken r at a time is 
$$
\begin{align*}
_{n}C_{r} &= \begin{pmatrix}n\\r\end{pmatrix} \\
&= \frac{_{n}P_{r}}{r!} \\
&= \frac{n!}{r!(n-r)!} \tag{3.2-2}\\ 
&= \frac{n(n-1)(n-2)...(n-r+1)}{r!}
\end{align*}
$$

### 3.3 Probability

**classical probability concept**: if there are m equally likely possibilities, of which one must occur, and s is regarded as favourable outcomes, then probability of success is given by $\frac{s}{m}$.

**frequency interpretation**: probability of an event/outcome is proportion of the times the event will occur in the long run of repeated experiments.
$$
r_N=\frac{\text{Number of occurances of A in N trials}}{\text{N}}
$$
**subjective probabilities** express strength of one's belief w.r.t. uncertainties involved. 

### 3.4 Axioms Probability

probability: a set of **additive set functions**

- a set function that assigns to each subset $A$ of a finite sample space $S$ the number of elements in $A$, written $N(A)$. 

conditions of the additive set function defining $P(A)$:

1. $0\leq P(A)\leq 1$ for each event $A$ in $S$.

   $P(A) = \frac{s}{m}$, where $0\leq s\leq m<1$

2. $P(S) = 1$.

   $P(S) = \frac{m}{m}=1$

3. if $A$ and $B$ are mutually exclusive events in $S$, then
   $$
   \begin{align*}
   P(A \cup B)&=\frac{s_1}{m} + \frac{s_2}{m} \\
   &= \frac{s_1+s_2}{m} \\
   &=P(A)+P(B)
   \end{align*}
   $$

$$
P(A)=\sum_{i=0}^{N}\ p_i \\
$$

where $N$ is the number of outcomes in $A$ and $p_i$ is the probability of the $i^{th}$ outcome.

### 3.5 Elementary Theorems

[thm] if $A_{1}, A_{2}, ..., A_{n}$ are mutually exclusive in a sample space S, then 
$$
P(A_1 \cup A_2 \cup ... \cup A_n)= P(A_1)+P(A_2) + ... + P(A_n) \tag{3.5-1}
$$
[thm] if $A$ is an event in the finite sample space $S$, then $P(A)$ equals the sum of probabilities of the individual outcomes comprising $A$.

[thm] if $A$ and $B$ are any events in $S$, then 
$$
\begin{align*}
P(A\cup B) &= P(A) + P(B) - P(A\cap B) \\
\text{proof: }
P(A\cup B) &= P(A\cap B) + P(A\cap \bar{B}) + P(\bar{A}\cap B) \\
&= [P(A\cap B)+P(A\cap \bar{B})]+[P(A\cap B)+P(\bar{A}\cap B)]-P(A\cap B) \\
&= P(A) + P(B) - P(A\cap B) \\
\end{align*}
$$
mutually exclusive, thus $P(A\cap B)=0$.

[thm] if $A$ is any event in $S$, then $P(\bar{A})=1-P(A)$.

### 3.6 Conditional Probability

if $A$ and $B$ are any events in $S$ and $P(B)\neq 0$, the conditional probability of $A$ given by B is
$$
P(A|B)= \frac{P(A\cap B)}{P(B)} \tag{3.6-1}\\
$$
[thm] if $A$ and $B$ are <u>independent</u> events in $S$, then
$$
\begin{align*}
P(A\cap B) &= P(A)\cdot P(B|A)\ &\text{if }P(A)\neq 0\\
&= P(B)\cdot P(A|B)\ &\text{if }P(B)\neq 0\\
\end{align*}
$$
[thm] 2 events $A$ and $B$ are independent events $iff$
$$
P(A\cap B) = P(A)\cdot P(B)
$$

### 3.7 Bayes' Theorem

$$
P(A)=\sum_{i=1}^{n}\ P(B_i)\cdot P(A|B_i) \tag{3.7-1} \\
$$

![image-20200522083843317](C:\Users\Kaycee\AppData\Roaming\Typora\typora-user-images\image-20200522083843317.png)

[thm] **Bayes' Theorem** (rule of total probability): if $B_1, B_2, ..., B_n$ are mutually exclusive events of which one must occur, then
$$
P(B_r|A) = \frac{P(B_r)\cdot P(A|B_r)}{\sum_{i=1}^n\ P(B_i)\cdot P(A|B_i)}
$$
for $r=1,2,...,n$ 